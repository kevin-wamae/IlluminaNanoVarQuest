# *********************************************************************
#           snakemake pipeline for fastqc quality control
# *********************************************************************


# *********************************************************************
#                             dependencies
# *********************************************************************
# configuration file
# ---------------------------------------------------------------------
configfile: "config/config.yaml"


# global wild cards of sample and matepair list
(SAMPLES,) = glob_wildcards(
    config['input']['fastq'] + "{sample}.fastq.gz")

# all output
# ---------------------------------------------------------------------
rule all:
    input:
        # ------------------------------------
        # qc_raw_files
        expand(
            config['fastqcRaw']['dir'] + '{sample}_fastqc.html', sample=SAMPLES),
        expand(
            config['fastqcRaw']['dir'] + '{sample}_fastqc.zip', sample=SAMPLES),
        config['fastqcRaw']['dir'],

        # ------------------------------------
        # merge_qc_reports
        # ------------------------------------
        config['multiqc']['dir'],

        # ------------------------------------
        # trim_ont_adapters
        # ------------------------------------
        config['porechop']['fastqRaw'],
        config['porechop']['fastqTrim'],
        config['porechop']['log'],

        # ------------------------------------
        # trim_illumina_adapters
        # ------------------------------------
        config['cutadapt']['fastqTrim'],
        config['cutadapt']['log'],

        # ------------------------------------
        # qc_trimmed_files
        config['dirQcTrim']['fastqc'],

        # ------------------------------------
        # bwa_index
        # ------------------------------------
        # location of genome index file
        config['bwa']['index'],

        # ------------------------------------
        # bwa_map
        # ------------------------------------
        # location of bam alignment file
        config['bam']['sorted'],
        # location of bam alignment file, duplicates marked
        config["bam"]['markDuplicates'],
        # location of bam file index
        config['bam']['index'],
        # location of bed file, raw
        config['bam']['bed'],


        # -------------------------------------
        # merge_features
        # ------------------------------------
        # location of bed file, overlapping intervals merged
        config['bed']['merged'],
        # location of bed file, genome coverage
        config['bed']['genomeCov'],
        # location of bed file, annotated with gff features
        config['bed']['gffToBed'],
        # location of annotated bed file
        config['bed']['annotated'],

        # ------------------------------------
        # variant_calling
        # ------------------------------------
        config['bcftools']['vcf'],

        # ------------------------------------
        # snpEff_annotate
        # ------------------------------------
        config['snpEff']['vcf'],

        # ------------------------------------
        # snpSift_extract
        # ------------------------------------
        config['snpSift']['annotations'],
        config['snpSift']['alleleFreq']


# *********************************************************************
# fastqc - check quality of raw fastq-files and merge fastqc reports
# *********************************************************************
rule qc_raw_files:
    input:
        fastq = expand(
            config['input']['fastq'] + "{sample}.fastq.gz", sample=SAMPLES)
    output:
        html = expand(
            config['fastqcRaw']['dir'] + '{sample}_fastqc.html', sample=SAMPLES),
        zip = expand(
            config['fastqcRaw']['dir'] + "{sample}_fastqc.zip", sample=SAMPLES),
        dir = directory(config['fastqcRaw']['dir'])
    run:
        shell(
            """
            fastqc {input.fastq} --format fastq --quiet --outdir {output.dir} | tee {log}
            """
        )


# *********************************************************************
# multiqc - merge fastqc reports
# *********************************************************************
rule merge_qc_reports:
    input:
        rules.qc_raw_files.output.dir
    output:
        dir = directory(config['multiqc']['dir'])
    shell:
        """
        multiqc -f {input} -o {output.dir} | tee {log}
        """

# *********************************************************************
# porechop - clip ONT adapters, start by merge fastq files
# *********************************************************************
rule trim_ont_adapters:
    input:
        rules.qc_raw_files.input
    output:
        fastqMerge = config['porechop']['fastqRaw'],
        fastqTrim = config['porechop']['fastqTrim'],
    log: config['porechop']['log']
    params:
        checkreads = 10000
    shell:
        """
        zcat {input} > {output.fastqMerge}
        porechop --check_reads {params.checkreads} -i {output.fastqMerge} -o {output.fastqTrim} | tee {log}
        """

# *********************************************************************
# cutadapt - clip illumina adapters
# *********************************************************************
rule trim_illumina_adapters:
    input:
        fastqIn = rules.trim_ont_adapters.output.fastqTrim,
        adapters = config['input']['adapters']
    output:
        fastqTrim = config['cutadapt']['fastqTrim'],
    log: config['cutadapt']['log']
    shell:
        """
        cutadapt \
            --minimum-length 100 \
            --discard-untrimmed \
            --front file:{input.adapters} \
            --quality-cutoff 20 \
            --errors 3 \
            -o {output.fastqTrim} \
            {input.fastqIn} \
            --report minimal \
            --json {log}
        """


# *********************************************************************
# fastqc - check quality of trimmed fastq-files
#   - unlike rule qc_raw_files, we have only one output since we are processing
#     a single fastq file
# *********************************************************************
rule qc_trimmed_files:
    input:
        rules.trim_illumina_adapters.output.fastqTrim
    output:
        dir = directory(config['dirQcTrim']['fastqc'])
    shell:
        """
        mkdir {output.dir}
        fastqc {input} --format fastq --quiet --outdir {output.dir} | tee --append {log}
        """

# *********************************************************************
# bwa - generate bwa genome-index files
# *********************************************************************
rule bwa_index:
    input:
        genomeFasta = config['input']['genome']['fasta'],
    output:
        genomeIndex = touch(config['bwa']['index'])
    shell:
        """
        bwa index -p {output.genomeIndex} {input.genomeFasta}
        """

# *********************************************************************
# bwa/samtools/sambamba:
#    - map reads to genome
#    - convert to bam and sort
#    - drop pcr duplicates
#    - index and convert bam to bed
# *********************************************************************
rule map_reads:
    input:
        genomeIndex = rules.bwa_index.output.genomeIndex,
        fastq = rules.trim_illumina_adapters.output.fastqTrim
    output:
        bamSorted = config['bam']['sorted'],
        bamMarkDup = config["bam"]['markDuplicates'],
        bamIndex = config['bam']['index'],
        bamToBed = config['bam']['bed']
    params:
        threads = 4
    run:
        shell(  # generate sorted bam files of short-read mapping
            """
            bwa mem -M -t {params.threads} {input.genomeIndex} {input.fastq} | \
            samtools view -Sb |\
            samtools sort -o {output.bamSorted}
            """
        )
        shell(  # sambamba - mark duplicates
            """
            sambamba markdup --show-progress --remove-duplicates \
                --nthreads={params.threads} {output.bamSorted} {output.bamMarkDup}
            """
        )
        shell(  # index bam files for downstream analysis
            """
            samtools index {output.bamMarkDup} {output.bamIndex}
            """
        )
        shell(  # convert bam file to bed format
            """
            bedtools bamtobed -cigar -i {output.bamMarkDup} > {output.bamToBed}
            """
        )


# *********************************************************************
# bedtools - merge overlapping intervals
#  -s: merge features that are on the same strand
#  -i: input (bed/gff/vcf)
#  -c: report strand from bed file
#  -o: specify operation (distinct removes duplicates)
# *********************************************************************
rule merge_features:
    input:
        bed = rules.map_reads.output.bamToBed,
        bam = rules.map_reads.output.bamMarkDup,
        gff = config['input']['genome']['gff']
    output:
        merged = config['bed']['merged'],
        bedCov = config['bed']['genomeCov'],
        gffToBed = config['bed']['gffToBed'],
        annotated = config['bed']['annotated']
    params:
        gffFilter = 'protein_coding_gene'
    run:
        shell(  # bedtools - merge overlapping intervals
            """
            bedtools merge -s -c 6,5 -o distinct,mean -i {input.bed} > {output.merged}
            """
        )
        shell(  # bedtools - compute coverate across genome features
            """
            bedtools genomecov -bg -ibam {input.bam} |\
            bedtools merge -c 4 -o median > {output.bedCov}
            """
        )
        shell(  # bedops - convert .gff to .bed using bedops and filter to exome
            """
            convert2bed --input=gff --output=bed < {input.gff} | \
            grep -e {params.gffFilter} > {output.gffToBed}
            """
        )
        shell(  # bedtools - annotate intervals in the bed file with features from
                # column 10 of .gff and collapse overlapping features
            """
            bedtools map -c 10 -o collapse -a {output.bedCov} -b {output.gffToBed} > {output.annotated}
            """
        )


# *********************************************************************
# bcftools - variant calling
#   - bcftools mpileup - generates genotype likelihoods at each genomic
#     position with coverage.
#   - bcftools call - makes the actual calls
#   - bcftools filter - drop variants with QUAL<=20 and Depth of Coverage
# *********************************************************************
rule variant_calling:
    input:
        genome = rules.bwa_index.input.genomeFasta,
        bam = rules.map_reads.output.bamMarkDup,
    output:
        bcftools_vcf = config['bcftools']['vcf']
    shell:
        """
        bcftools mpileup \
            --annotate FORMAT/AD,FORMAT/ADF,FORMAT/ADR,FORMAT/SP,INFO/AD,INFO/ADF,INFO/ADR \
            -f {input.genome} {input.bam} |\
        bcftools call --skip-variants indels --multiallelic-caller --variants-only |\
        bcftools filter -s LowQual -e 'QUAL<20 || DP<100' --output-type z --output {output.bcftools_vcf}
        """

# *********************************************************************
# snpEff - variant annotation and functional effect prediction
# *********************************************************************
rule annotate_variants:
    input:
        rules.variant_calling.output.bcftools_vcf
    output:
        snpEff_vcf = config['snpEff']['vcf']
    params:
        config = config['snpEff']['config'],
        database = config['snpEff']['database']
    shell:
        """
        snpEff -no-downstream -no-intergenic -no-intron -no-upstream \
               -no-utr -hgvs1LetterAa -noLof -noShiftHgvs -noMotif \
               -no SPLICE_SITE_REGION -noInteraction -noStats  \
               -config {params.config} {params.database}  {input} | gzip > {output.snpEff_vcf}
        """


# *********************************************************************
# SnpSift - extract vcf fields
# *********************************************************************
rule extract_variants:
    input:
        rules.annotate_variants.output.snpEff_vcf
    output:
        tableAnnot = config['snpSift']['annotations'],
        tableAlleleFreq = config['snpSift']['alleleFreq']
    run:
        shell(  # SnpSift - extract vcf fields
                """
                 SnpSift extractFields {input} \
                    CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" \
                    "ANN[*].GENEID" "ANN[*].HGVS_C" "ANN[*].HGVS_P" \
                    "ANN[*].CDS_POS" "ANN[*].AA_POS" "GEN[*].AD" > {output.tableAnnot}
                """
        )
        shell(  # compute allele frequencies from AD column of .vcf file
            """
            awk '
                BEGIN {{ FS=OFS="\\t" }}
                NR == 1 {{
                    allelFreq1 = "AF_REF"
                    allelFreq2 = "AF_ALT"
                }}
                NR > 1 {{
                    split($12,a,",")
                    sum = a[1] + a[2]
                    if ( sum ) {{
                        allelFreq1 = a[1] / sum
                        allelFreq2 = a[2] / sum
                    }}
                    else {{
                        allelFreq1 = 0
                        allelFreq2 = 0
                    }}
                }}
                {{ print $0, allelFreq1, allelFreq2 }}
            ' {output.tableAnnot} | sed --expression 's/ANN\\[\\*\\]\\.\\|GEN\\[\\*\\]\\.//g' > {output.tableAlleleFreq}
            """
        )
